{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.7.7","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"},"notebookId":"d0a7e4cf-c529-4e71-98d4-d1644e89e0e9","kernelspec":{"name":"python3","description":"IPython kernel implementation for Yandex DataSphere","spec":{"language":"python","display_name":"Yandex DataSphere Kernel","codemirror_mode":"python","argv":["/bin/true"],"env":{},"help_links":[]},"resources":{},"display_name":"Yandex DataSphere Kernel"},"ydsNotebookPath":"[homework]classification.ipynb"},"cells":[{"cell_type":"markdown","source":"<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>","metadata":{"cellId":"794f3ec0-ed1c-428d-8ca2-e016411c5c97"}},{"cell_type":"markdown","source":"---","metadata":{"cellId":"046fed75-1861-4c0a-9fdf-fbb6074c8a35"}},{"cell_type":"markdown","source":"# Задание 3\n\n## Классификация текстов\n\nВ этом задании вам предстоит попробовать несколько методов, используемых в задаче классификации, а также понять насколько хорошо модель понимает смысл слов и какие слова в примере влияют на результат.","metadata":{"cellId":"f586354a-a5b4-4594-af12-958524e42267"}},{"cell_type":"code","source":"#!g1.1\n# %pip install torchtext","metadata":{"cellId":"m9hg8ic8n098dc87g02kxb"},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchtext in /home/jupyter/.local/lib/python3.7/site-packages (0.8.1)\nRequirement already satisfied: numpy in /kernel/lib/python3.7/site-packages (from torchtext) (1.19.4)\nRequirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from torchtext) (2.25.1)\nRequirement already satisfied: torch==1.7.1 in /home/jupyter/.local/lib/python3.7/site-packages (from torchtext) (1.7.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.50.0)\nRequirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from torch==1.7.1->torchtext) (3.10.0.2)\nRequirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->torchtext) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->torchtext) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.7/site-packages (from requests->torchtext) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->torchtext) (2021.10.8)\n"}],"execution_count":18},{"cell_type":"code","source":"#!g1.1\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom torchtext import datasets\n\nfrom torchtext.data import Field, LabelField\nfrom torchtext.data import BucketIterator\n\nfrom torchtext.vocab import Vectors, GloVe\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport random\nfrom tqdm.autonotebook import tqdm","metadata":{"cellId":"0dbcf5cc-c7f3-414b-a449-b8f5190ac34a","trusted":true},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":"### В этом задании мы будем использовать библиотеку torchtext. Она довольна проста в использовании и поможет нам сконцентрироваться на задаче, а не на написании Dataloader-а.","metadata":{"cellId":"cc05951b-0cd1-4c9f-b8eb-ca9504d35fd6"}},{"cell_type":"code","source":"#!g1.1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"cellId":"7bgxpkpamfljs4u03ehcip","trusted":true},"outputs":[],"execution_count":96},{"cell_type":"code","source":"#!g1.1\nTEXT = Field(sequential=True, lower=True, include_lengths=True)  # Поле текста\nLABEL = LabelField(dtype=torch.float)  # Поле метки","metadata":{"cellId":"a1f086e0-2c1d-4914-bdcc-2999d25bf85b","trusted":true},"outputs":[],"execution_count":97},{"cell_type":"code","source":"#!g1.1\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"cellId":"cc85eb4d-e20c-42d3-a730-fe0d5533575a","trusted":true},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":"Датасет на котором мы будем проводить эксперементы это комментарии к фильмам из сайта IMDB.","metadata":{"cellId":"fc96d7c6-c0e2-41a8-aec0-335b1a52a4e6"}},{"cell_type":"code","source":"#!g1.1\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)  # загрузим датасет\ntrain, valid = train.split(random_state=random.seed(SEED))  # разобьем на части","metadata":{"cellId":"c8590b54-73d4-4f7e-ae0f-458508183d28","trusted":true},"outputs":[],"execution_count":99},{"cell_type":"code","source":"#!g1.1\nfor x in train:\n    print(x)\n    break","metadata":{"cellId":"bx2orh7kmkh1weus2vt725","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"<torchtext.data.example.Example object at 0x7ff34e787f50>\n"}],"execution_count":100},{"cell_type":"code","source":"#!g1.1\nTEXT.build_vocab(train)\nLABEL.build_vocab(train)","metadata":{"cellId":"505234b9-f3e1-4174-a8f8-056474bd2d64","trusted":true},"outputs":[],"execution_count":101},{"cell_type":"code","source":"#!g1.1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_iter, valid_iter, test_iter = BucketIterator.splits(\n    (train, valid, test), \n    batch_size = 64,\n    sort_within_batch = True,\n    device = device)","metadata":{"cellId":"6c074439-04d5-42c1-bb20-13b4aec5cd31","trusted":true},"outputs":[],"execution_count":102},{"cell_type":"code","source":"#!g1.1\nnum_classes = len(LABEL.vocab)","metadata":{"cellId":"vg9gfadqr6a2tr9m989of","trusted":true},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":"## RNN\n\nДля начала попробуем использовать рекурентные нейронные сети. На семинаре вы познакомились с GRU, вы можете также попробовать LSTM. Можно использовать для классификации как hidden_state, так и output последнего токена.","metadata":{"cellId":"9014c9de-1698-43a1-84a6-b64a74464cad"}},{"cell_type":"code","source":"#!g1.1\nclass RNNBaseline(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(input_size=embedding_dim, \n                           hidden_size=hidden_dim, \n                           bidirectional=bidirectional,\n                           num_layers=n_layers,\n                           dropout=dropout)  # YOUR CODE GOES HERE\n        self.fc = nn.Linear(2 * n_layers * hidden_dim, num_classes)  # YOUR CODE GOES HERE\n        \n        \n    def forward(self, text, text_lengths):\n        \n        #text = [sent len, batch size]\n        \n        embedded = self.embedding(text)\n        \n        #embedded = [sent len, batch size, emb dim]\n        \n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        # cell arg for LSTM, remove for GRU\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        #unpack sequence\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)  \n\n        #output = [sent len, batch size, hid dim * num directions]\n        #output over padding tokens are zero tensors\n        \n        #hidden = [num layers * num directions, batch size, hid dim]\n        #cell = [num layers * num directions, batch size, hid dim]\n        \n        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        #and apply dropout\n        print('hidden[-1,:,:].shape =', hidden[-1,:,:].shape)\n        print('hidden[-2,:,:].shape =', hidden[-2,:,:].shape)\n        hidden = torch.stack((hidden[-1,:,:], hidden[-2,:,:]), dim=0)  # YOUR CODE GOES HERE\n        print('hidden.shape =', hidden.shape)    \n        #hidden = [batch size, hid dim * num directions] or [batch_size, hid dim * num directions]\n            \n        return self.fc(hidden)","metadata":{"cellId":"166980a5-67c6-4d25-8432-7f0c7a930283","trusted":true},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":"Поиграйтесь с гиперпараметрами","metadata":{"cellId":"3f7b2210-8e0a-41f9-b419-a4938eedc06e"}},{"cell_type":"code","source":"#!g1.1\nvocab_size = len(TEXT.vocab)\nemb_dim = 100\nhidden_dim = 256\noutput_dim = 1\nn_layers = 2\nbidirectional = True\ndropout = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\npatience=3","metadata":{"cellId":"e9974b9e-def5-4d54-b45b-8aba11ddc9dd","trusted":true},"outputs":[],"execution_count":105},{"cell_type":"code","source":"#!g1.1\nmodel = RNNBaseline(\n    vocab_size=vocab_size,\n    embedding_dim=emb_dim,\n    hidden_dim=hidden_dim,\n    output_dim=output_dim,\n    n_layers=n_layers,\n    bidirectional=bidirectional,\n    dropout=dropout,\n    pad_idx=PAD_IDX\n)","metadata":{"cellId":"ac453e4c-8abe-4719-97c6-3e396968d3b3","trusted":true},"outputs":[],"execution_count":106},{"cell_type":"code","source":"#!g1.1\nmodel = model.to(device)","metadata":{"cellId":"97b4587b-3634-4d5e-a748-521aefed5d71","trusted":true},"outputs":[],"execution_count":107},{"cell_type":"code","source":"#!g1.1\nopt = torch.optim.Adam(model.parameters())\nloss_func = nn.BCEWithLogitsLoss()\n\nmax_epochs = 20","metadata":{"cellId":"0e3d1a8c-845d-4680-bba7-b7e7db1b9b24","trusted":true},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":"Обучите сетку! Используйте любые вам удобные инструменты, Catalyst, PyTorch Lightning или свои велосипеды.","metadata":{"cellId":"90301b5e-ea8c-41e8-a999-1a56fce6e989"}},{"cell_type":"code","source":"#!g1.1\nfor x in train_iter:\n    print(x)\n    break","metadata":{"cellId":"3rvplwqc1kwzuja3l6y79p","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"\n[torchtext.data.batch.Batch of size 64]\n\t[.text]:('[torch.cuda.LongTensor of size 81x64 (GPU 0)]', '[torch.cuda.LongTensor of size 64 (GPU 0)]')\n\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n"}],"execution_count":117},{"cell_type":"code","source":"#!g1.1\nimport numpy as np\n\nmin_loss = np.inf\n\ncur_patience = 0\n\nfor epoch in range(1, max_epochs + 1):\n    train_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar: \n        #YOUR CODE GOES HERE\n        \n        X_batch = batch.text.to(device)\n        y_batch = batch.label.to(device)\n        \n        y_pred = model(X_batch)\n        \n\n    train_loss /= len(train_iter)\n    val_loss = 0.0\n    model.eval()\n    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar:\n        # YOUR CODE GOES HERE\n    val_loss /= len(valid_iter)\n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_model = model.state_dict()\n    else:\n        cur_patience += 1\n        if cur_patience == patience:\n            cur_patience = 0\n            break\n    \n    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\nmodel.load_state_dict(best_model)","metadata":{"cellId":"8d1c1ae6-dc20-489a-a4a7-2ea453b39764"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора на тестовом датасете.\n\n**Ответ**:","metadata":{"cellId":"370d0f53-9ade-44b9-9a0a-f47bf2dd70ba"}},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"bb0075ef-191d-4ec3-b3af-2f4e7031ce28"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CNN\n\n![](https://www.researchgate.net/publication/333752473/figure/fig1/AS:769346934673412@1560438011375/Standard-CNN-on-text-classification.png)\n\nДля классификации текстов также часто используют сверточные нейронные сети. Идея в том, что как правило сентимент содержат словосочетания из двух-трех слов, например \"очень хороший фильм\" или \"невероятная скука\". Проходясь сверткой по этим словам мы получим какой-то большой скор и выхватим его с помощью MaxPool. Далее идет обычная полносвязная сетка. Важный момент: свертки применяются не последовательно, а параллельно. Давайте попробуем!","metadata":{"cellId":"3833b5e6-d836-4aca-be78-ee9921c1bfd7"}},{"cell_type":"code","source":"#!g1.1\nTEXT = Field(sequential=True, lower=True, batch_first=True)  # batch_first тк мы используем conv  \nLABEL = LabelField(batch_first=True, dtype=torch.float)\n\ntrain, tst = datasets.IMDB.splits(TEXT, LABEL)\ntrn, vld = train.split(random_state=random.seed(SEED))\n\nTEXT.build_vocab(trn)\nLABEL.build_vocab(trn)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"cellId":"3cde3aca-d6ba-4e07-860d-17503fddee32"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ntrain_iter, val_iter, test_iter = BucketIterator.splits(\n        (trn, vld, tst),\n        batch_sizes=(128, 256, 256),\n        sort=False,\n        sort_key= lambda x: len(x.src),\n        sort_within_batch=False,\n        device=device,\n        repeat=False,\n)","metadata":{"cellId":"81236a31-cc21-41eb-b167-d553cf347a6d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вы можете использовать Conv2d с `in_channels=1, kernel_size=(kernel_sizes[0], emb_dim))` или Conv1d c `in_channels=emb_dim, kernel_size=kernel_size[0]`. Но хорошенько подумайте над shape в обоих случаях.","metadata":{"cellId":"191cc767-03d9-47e9-b755-40c9118319f4"}},{"cell_type":"code","source":"#!g1.1\nclass CNN(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        emb_dim,\n        out_channels,\n        kernel_sizes,\n        dropout=0.5,\n    ):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.conv_0 = None  # YOUR CODE GOES HERE\n        \n        self.conv_1 = None  # YOUR CODE GOES HERE\n        \n        self.conv_2 = None  # YOUR CODE GOES HERE\n        \n        self.fc = nn.Linear(len(kernel_sizes) * out_channels, 1)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        \n    def forward(self, text):\n        \n        embedded = self.embedding(text)\n        \n        embedded = embedded  # may be reshape here\n        \n        conved_0 = F.relu(self.conv_0(embedded))  # may be reshape here\n        conved_1 = F.relu(self.conv_1(embedded))  # may be reshape here\n        conved_2 = F.relu(self.conv_2(embedded))  # may be reshape here\n        \n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n        \n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n            \n        return self.fc(cat)","metadata":{"cellId":"5d042b45-4970-4fc8-bcc5-538c84630f95"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nkernel_sizes = [3, 4, 5]\nvocab_size = len(TEXT.vocab)\nout_channels=64\ndropout = 0.5\ndim = 300\n\nmodel = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=out_channels,\n            kernel_sizes=kernel_sizes, dropout=dropout)","metadata":{"cellId":"f91a6df3-c792-49c7-960b-9f468e5f9f80"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nmodel.to(device)","metadata":{"cellId":"4469ae3a-7674-4660-a282-39f73f67f8e7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nopt = torch.optim.Adam(model.parameters())\nloss_func = nn.BCEWithLogitsLoss()","metadata":{"cellId":"437e7582-774a-4c72-8a1a-eec6f021ccef"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nmax_epochs = 30","metadata":{"cellId":"a377b6dd-1470-4d78-bad9-0d5ca5aec96e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Обучите!","metadata":{"cellId":"d501a25d-6bf6-4205-ab77-a423a503dd6d"}},{"cell_type":"code","source":"#!g1.1\nimport numpy as np\n\nmin_loss = np.inf\n\ncur_patience = 0\n\nfor epoch in range(1, max_epochs + 1):\n    train_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar: \n        #YOUR CODE GOES HERE\n\n    train_loss /= len(train_iter)\n    val_loss = 0.0\n    model.eval()\n    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar:\n        # YOUR CODE GOES HERE\n    val_loss /= len(valid_iter)\n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_model = model.state_dict()\n    else:\n        cur_patience += 1\n        if cur_patience == patience:\n            cur_patience = 0\n            break\n    \n    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\nmodel.load_state_dict(best_model)","metadata":{"cellId":"2cea8088-075c-4b4f-b91b-d09c4ebfc856"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора.\n\n**Ответ**:","metadata":{"cellId":"194fbdd3-757b-43df-9c44-2e0db714a6a8"}},{"cell_type":"markdown","source":"## Интерпретируемость\n\nПосмотрим, куда смотрит наша модель. Достаточно запустить код ниже.","metadata":{"cellId":"09beb23c-32ef-4268-b9a2-14a8d322edda"}},{"cell_type":"code","source":"#!g1.1\n!pip install -q captum","metadata":{"cellId":"33c67e85-9e32-4364-b147-a60b244675a5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n\nPAD_IND = TEXT.vocab.stoi['pad']\n\ntoken_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\nlig = LayerIntegratedGradients(model, model.embedding)","metadata":{"cellId":"dc6f7f2c-04d3-4258-9828-3f4a44ffa2d4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndef forward_with_softmax(inp):\n    logits = model(inp)\n    return torch.softmax(logits, 0)[0][1]\n\ndef forward_with_sigmoid(input):\n    return torch.sigmoid(model(input))\n\n\n# accumalate couple samples in this array for visualization purposes\nvis_data_records_ig = []\n\ndef interpret_sentence(model, sentence, min_len = 7, label = 0):\n    model.eval()\n    text = [tok for tok in TEXT.tokenize(sentence)]\n    if len(text) < min_len:\n        text += ['pad'] * (min_len - len(text))\n    indexed = [TEXT.vocab.stoi[t] for t in text]\n\n    model.zero_grad()\n\n    input_indices = torch.tensor(indexed, device=device)\n    input_indices = input_indices.unsqueeze(0)\n    \n    # input_indices dim: [sequence_length]\n    seq_length = min_len\n\n    # predict\n    pred = forward_with_sigmoid(input_indices).item()\n    pred_ind = round(pred)\n\n    # generate reference indices for each sample\n    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n\n    # compute attributions and approximation delta using layer integrated gradients\n    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n                                           n_steps=5000, return_convergence_delta=True)\n\n    print('pred: ', LABEL.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n\n    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n    \ndef add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n    attributions = attributions.sum(dim=2).squeeze(0)\n    attributions = attributions / torch.norm(attributions)\n    attributions = attributions.cpu().detach().numpy()\n\n    # storing couple samples in an array for visualization purposes\n    vis_data_records.append(visualization.VisualizationDataRecord(\n                            attributions,\n                            pred,\n                            LABEL.vocab.itos[pred_ind],\n                            LABEL.vocab.itos[label],\n                            LABEL.vocab.itos[1],\n                            attributions.sum(),       \n                            text,\n                            delta))","metadata":{"cellId":"b58f4a63-1a00-41be-97a1-f9c23b23fc92"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ninterpret_sentence(model, 'It was a fantastic performance !', label=1)\ninterpret_sentence(model, 'Best film ever', label=1)\ninterpret_sentence(model, 'Such a great show!', label=1)\ninterpret_sentence(model, 'It was a horrible movie', label=0)\ninterpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\ninterpret_sentence(model, 'It is a disgusting movie!', label=0)","metadata":{"cellId":"abd2aa6d-ca46-403b-9844-570485f3d0f9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Попробуйте добавить свои примеры!","metadata":{"cellId":"22c338ef-ab66-44c4-aead-42afb53bcaf6"}},{"cell_type":"code","source":"#!g1.1\nprint('Visualize attributions based on Integrated Gradients')\nvisualization.visualize_text(vis_data_records_ig)","metadata":{"cellId":"eac85f47-e10e-4103-b710-4e71e414c4f0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Эмбеддинги слов\n\nВы ведь не забыли, как мы можем применить знания о word2vec и GloVe. Давайте попробуем!","metadata":{"cellId":"6268c7d2-28d7-44e3-b369-90179b84666b"}},{"cell_type":"code","source":"#!g1.1\nTEXT.build_vocab(trn, vectors=)# YOUR CODE GOES HERE\n# подсказка: один из импортов пока не использовался, быть может он нужен в строке выше :)\nLABEL.build_vocab(trn)\n\nword_embeddings = TEXT.vocab.vectors\n\nkernel_sizes = [3, 4, 5]\nvocab_size = len(TEXT.vocab)\ndropout = 0.5\ndim = 300","metadata":{"cellId":"d08b0ff5-ed93-46cd-822c-69f7234bcae3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ntrain, tst = datasets.IMDB.splits(TEXT, LABEL)\ntrn, vld = train.split(random_state=random.seed(SEED))\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        return self.fc(hidden)\ntrain_iter, val_iter, test_iter = BucketIterator.splits(\n        (trn, vld, tst),\n        batch_sizes=(128, 256, 256),\n        sort=False,\n        sort_key= lambda x: len(x.src),\n        sort_within_batch=False,\n        device=device,\n        repeat=False,\n)","metadata":{"cellId":"35955d93-3e95-4e9a-8b55-5dc992509893"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nmodel = CNN(vocab_size=vocab_size, emb_dim=dim, out_channels=64,\n            kernel_sizes=kernel_sizes, dropout=dropout)\n\nword_embeddings = TEXT.vocab.vectors\n\nprev_shape = model.embedding.weight.shape\n\nmodel.embedding.weight = # инициализируйте эмбэдинги\n\nassert prev_shape == model.embedding.weight.shape\nmodel.to(device)\n\nopt = torch.optim.Adam(model.parameters())","metadata":{"cellId":"b9da61a3-d3bb-4b91-a495-fd3fe3ca2c62"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вы знаете, что делать.","metadata":{"cellId":"801f9c88-bb0e-4241-a8d9-b58b71341fcb"}},{"cell_type":"code","source":"#!g1.1\nimport numpy as np\n\nmin_loss = np.inf\n\ncur_patience = 0\n\nfor epoch in range(1, max_epochs + 1):\n    train_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar: \n        #YOUR CODE GOES HERE\n\n    train_loss /= len(train_iter)\n    val_loss = 0.0\n    model.eval()\n    pbar = tqdm(enumerate(valid_iter), total=len(valid_iter), leave=False)\n    pbar.set_description(f\"Epoch {epoch}\")\n    for it, batch in pbar:\n        # YOUR CODE GOES HERE\n    val_loss /= len(valid_iter)\n    if val_loss < min_loss:\n        min_loss = val_loss\n        best_model = model.state_dict()\n    else:\n        cur_patience += 1\n        if cur_patience == patience:\n            cur_patience = 0\n            break\n    \n    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))\nmodel.load_state_dict(best_model)","metadata":{"cellId":"e7c68cc5-8060-495f-8b69-9f6be583c802"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Посчитайте f1-score вашего классификатора.\n\n**Ответ**:","metadata":{"cellId":"19e6d2a0-e22e-489a-b7ab-19e019c45b6b"}},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"414dca6e-4407-46f4-bfba-e030715c98e0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проверим насколько все хорошо!","metadata":{"cellId":"c93e1f6c-df7e-4915-b2d8-e9b9fe13dea3"}},{"cell_type":"code","source":"#!g1.1\nPAD_IND = TEXT.vocab.stoi['pad']\n\ntoken_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\nlig = LayerIntegratedGradients(model, model.embedding)\nvis_data_records_ig = []\n\ninterpret_sentence(model, 'It was a fantastic performance !', label=1)\ninterpret_sentence(model, 'Best film ever', label=1)\ninterpret_sentence(model, 'Such a great show!', label=1)\ninterpret_sentence(model, 'It was a horrible movie', label=0)\ninterpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\ninterpret_sentence(model, 'It is a disgusting movie!', label=0)","metadata":{"cellId":"ba54e8eb-a281-4892-ab3f-280cdd4576d8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nprint('Visualize attributions based on Integrated Gradients')\nvisualization.visualize_text(vis_data_records_ig)","metadata":{"cellId":"be7a64fe-0c3f-47e0-b180-d4a587c11c3d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n\nfrom catboost import CatBoost, Pool\n\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.logger import JSONLogger\nfrom bayes_opt.event import Events\n\ndef validate_params(params):\n    params['iterations'] = int(params['iterations'])\n    params['depth'] = int(params['depth'])\n    params['max_ctr_complexity'] = int(params['max_ctr_complexity'])\n    params['bagging_temperature'] = int(params['bagging_temperature'])\n    \n#     weight = params['class_weights']\n#     if isinstance(weight, list):\n#         params['class_weights'] = weight\n#     else:\n#         params['class_weights'] = [1 / (1 + weight), weight/(1 + weight)]\n    return params\n\ndef optimize_catboost(train_pool, validation_pool, verbose=1, task_type = \"CPU\"):\n    \n    def get_model(**params):\n        return CatBoost(params)\n\n    def validate(model, train_pool, validation_pool, verbose=verbose,n = 5):\n        \n        if verbose:\n            display.clear_output(True)\n        \n        model.fit(train_pool, eval_set=validation_pool,\n              plot=verbose,\n              verbose = 0\n        )\n        \n        current_score = np.array(model.eval_metrics(validation_pool, metrics=\"NDCG\")['NDCG:type=Base']).max()\n        return current_score\n\n    def evaluate_model(**params):\n        params_init = {\n            'iterations': 200, \n            'verbose': 0,\n            \"task_type\": task_type,\n            'use_best_model': True,\n            'early_stopping_rounds': 100,\n            'eval_metric': 'NDCG',\n            'loss_function':'YetiRank',\n            'has_time': True\n        }\n        params_init.update(params)\n        params_init = validate_params(params_init)\n        model = get_model(**params_init)\n        current_score = validate(model, train_pool, validation_pool)\n        return current_score\n    \n    \n    pbounds = {\n        'iterations' : (50, 1000),\n        'depth': (4, 8),\n        'l2_leaf_reg': (1e-3, 1e2),\n        'random_strength': (1e-2, 10),\n        'bagging_temperature': (0, 10),\n        \"learning_rate\" : (0.0001, 0.5),\n        \"model_size_reg\" :(1e-2, 10),\n        'max_ctr_complexity' : (4, 15),\n#         'class_weights': (0.01, 100.0)\n\n    }\n    \n    optimizer = BayesianOptimization(evaluate_model, pbounds, random_state=4)\n\n    logger = JSONLogger(path=\"./logs.json\")\n    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n\n    optimizer.maximize(init_points=5, n_iter=20)\n\n    optimizer.res\n    \n    return optimizer.res","metadata":{"cellId":"b3032ac8-4b7c-4ca8-8322-aeaa8b1dadfe"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nrnn = nn.LSTM(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\nc0 = torch.randn(2, 3, 20)\noutput, (hn, cn) = rnn(input, (h0, c0))","metadata":{"cellId":"n4hq2m7655ayhtl3vwwden"},"outputs":[],"execution_count":54},{"cell_type":"code","source":"#!g1.1\noutput.shape, (hn.shape, cn.shape)","metadata":{"cellId":"e68ue1v42s7x72xtiijg0b"},"outputs":[{"output_type":"display_data","data":{"text/plain":"(torch.Size([5, 3, 20]), (torch.Size([2, 3, 20]), torch.Size([2, 3, 20])))"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"#!g1.1\noutput, (hn, cn)","metadata":{"cellId":"v9iaxtu3wkox7uc93bvns"},"outputs":[{"output_type":"display_data","data":{"text/plain":"(tensor([[[ 1.2710e-01,  2.3407e-01, -2.3605e-01,  8.2473e-02,  2.0139e-01,\n           -1.9939e-01,  2.9353e-02, -3.3229e-01,  1.4596e-01, -5.9030e-03,\n            1.7584e-01,  1.7773e-01,  1.5655e-01, -7.0125e-02, -2.7016e-01,\n           -2.8067e-01,  3.5292e-01, -2.4243e-01, -3.0099e-01,  5.9667e-02],\n          [ 1.1651e-01,  2.9497e-01,  2.7596e-01,  2.3363e-01, -4.9152e-02,\n            1.9111e-01,  1.0190e-01, -1.2395e-02, -3.1250e-01, -3.3005e-01,\n            1.7936e-01,  1.4203e-01,  2.0297e-01,  1.5817e-01, -2.4557e-01,\n           -1.7457e-01, -4.8433e-02,  3.2268e-01,  6.1570e-02,  2.3806e-01],\n          [ 3.5014e-01, -5.2389e-01, -3.0395e-01,  5.0340e-02,  1.8218e-01,\n           -1.6893e-01,  5.6530e-01, -1.1506e-01,  3.3178e-02, -3.1047e-01,\n            2.2037e-01,  6.0262e-02,  2.5250e-01, -3.1353e-01,  1.1230e-01,\n           -1.2776e-01,  6.4458e-01, -2.7705e-02,  2.2556e-01,  1.1571e-01]],\n \n         [[ 6.4016e-02,  1.2162e-01,  1.3032e-02,  1.4196e-02,  3.0950e-02,\n           -1.3930e-01, -4.6008e-03, -1.9337e-01, -5.7197e-03,  9.9844e-03,\n            1.7358e-01,  4.8016e-02,  1.3426e-01, -9.6923e-02, -5.0598e-02,\n           -2.2579e-01,  2.2915e-01, -4.9773e-02, -2.4177e-01,  1.0532e-01],\n          [ 1.1996e-01,  2.3918e-01,  2.7757e-01,  2.7504e-02, -1.1217e-01,\n            1.5360e-01,  1.3790e-01, -4.0155e-02, -2.2308e-01, -1.5493e-01,\n            1.0812e-01,  4.6574e-02,  1.1524e-01,  6.4450e-02,  2.6696e-02,\n           -1.4619e-01, -8.0872e-02,  2.4194e-01,  8.0834e-02,  8.1810e-02],\n          [ 8.7290e-02, -2.1991e-01, -1.3506e-01, -8.5171e-02,  1.0415e-02,\n           -1.3825e-01,  3.3096e-01, -1.0114e-01, -5.3582e-02, -3.1488e-01,\n            1.5557e-01,  1.2697e-02,  2.1825e-01, -2.5594e-01,  1.9335e-01,\n           -1.2610e-01,  2.8800e-01, -5.2355e-02,  1.1611e-01,  2.4478e-02]],\n \n         [[ 3.8906e-02,  7.4144e-02,  1.0717e-01, -1.9994e-02, -2.9653e-02,\n           -1.0555e-01,  2.3956e-02, -1.0429e-01, -5.2419e-02,  3.5270e-02,\n            1.5017e-01,  4.4154e-03,  1.0423e-01, -1.1975e-01,  4.8152e-02,\n           -1.6989e-01,  1.4684e-01,  1.7518e-02, -1.0722e-01,  6.2191e-02],\n          [ 9.2152e-02,  1.1823e-01,  2.3412e-01, -7.9687e-02, -1.0081e-01,\n            9.4126e-02,  1.3049e-01, -7.3076e-02, -1.9602e-01, -9.2094e-02,\n            6.9516e-02,  9.4327e-03,  1.2189e-01, -6.8609e-03,  1.3224e-01,\n           -1.1008e-01, -7.7355e-02,  1.8607e-01,  5.6029e-02,  1.4248e-02],\n          [ 4.9076e-02, -7.1645e-02, -5.6025e-02, -1.1833e-01, -5.0307e-02,\n           -1.0930e-01,  2.4239e-01, -9.6594e-02, -9.6482e-02, -1.8496e-01,\n            1.4200e-01, -2.2322e-02,  1.4875e-01, -1.9898e-01,  2.0120e-01,\n           -1.2483e-01,  1.2318e-01, -5.1930e-02,  2.8826e-02, -2.1701e-02]],\n \n         [[ 7.1568e-03,  3.3394e-03,  1.8467e-01, -4.3123e-02, -5.0411e-02,\n           -7.1910e-02,  4.7645e-02, -6.1531e-02, -8.1429e-02,  5.5160e-02,\n            1.4620e-01,  1.9976e-03,  7.9889e-02, -1.2587e-01,  8.4054e-02,\n           -1.1160e-01,  1.2599e-01,  4.7368e-02, -4.3490e-02, -1.7005e-03],\n          [ 6.8972e-02,  6.4056e-02,  2.1934e-01, -1.1141e-01, -9.2975e-02,\n            6.0745e-02,  1.4121e-01, -1.0373e-01, -1.6007e-01, -4.6794e-02,\n            4.9858e-02, -9.1861e-03,  1.2314e-01, -3.6776e-02,  1.7386e-01,\n           -9.8086e-02, -6.7447e-02,  1.2547e-01,  3.7262e-02, -2.9366e-02],\n          [ 3.7737e-02,  2.8751e-03,  2.9139e-02, -1.1659e-01, -6.5788e-02,\n           -8.4239e-02,  1.7804e-01, -1.0157e-01, -9.8675e-02, -1.0631e-01,\n            1.4149e-01, -3.3598e-02,  1.1409e-01, -1.6390e-01,  1.7734e-01,\n           -1.1414e-01,  6.2568e-02, -2.0249e-02,  6.2656e-03, -3.6107e-02]],\n \n         [[ 2.6977e-02, -5.1754e-03,  1.8601e-01, -9.8363e-02, -5.1774e-02,\n           -6.7844e-02,  6.8408e-02, -4.8830e-02, -9.9487e-02,  6.8405e-02,\n            1.3106e-01, -1.5307e-02,  1.2722e-01, -1.3097e-01,  1.2007e-01,\n           -8.1956e-02,  5.3994e-02,  7.2943e-02, -2.5930e-02, -3.2710e-02],\n          [ 6.2041e-02,  3.0493e-02,  2.0594e-01, -1.0421e-01, -8.0144e-02,\n            4.0276e-02,  1.3500e-01, -1.1821e-01, -1.2576e-01, -1.7198e-02,\n            5.4423e-02, -2.4828e-02,  1.3895e-01, -5.7857e-02,  1.7746e-01,\n           -8.7798e-02, -4.9159e-02,  9.6573e-02, -5.6701e-04, -6.3820e-02],\n          [ 5.5601e-02,  3.2263e-02,  8.3747e-02, -1.3286e-01, -7.3862e-02,\n           -1.0431e-01,  1.6283e-01, -9.1760e-02, -1.1460e-01, -3.5156e-02,\n            1.4335e-01, -4.3360e-02,  1.0741e-01, -1.4733e-01,  1.6162e-01,\n           -1.0434e-01,  2.9562e-02, -5.8561e-03, -7.5424e-03, -4.7616e-02]]],\n        grad_fn=<StackBackward>),\n (tensor([[[ 0.1324,  0.0736,  0.1552,  0.0743,  0.0668,  0.0583,  0.0338,\n            -0.1246, -0.2640, -0.0736, -0.0892, -0.1301, -0.0543, -0.0366,\n             0.1169,  0.1818, -0.0891,  0.1134, -0.0924,  0.0243],\n           [ 0.0260, -0.1147,  0.2453,  0.1071,  0.0116,  0.2412, -0.0439,\n            -0.0350,  0.0080,  0.0478, -0.1844,  0.1385, -0.2447, -0.1106,\n             0.1167,  0.0270, -0.1253,  0.0097, -0.1167,  0.0624],\n           [ 0.2191,  0.0776,  0.1925, -0.0251, -0.1461,  0.0632,  0.1151,\n             0.0213, -0.2095,  0.0254,  0.0700, -0.0592, -0.0608, -0.0231,\n             0.1424,  0.2216,  0.0724,  0.2027, -0.1837, -0.0576]],\n  \n          [[ 0.0270, -0.0052,  0.1860, -0.0984, -0.0518, -0.0678,  0.0684,\n            -0.0488, -0.0995,  0.0684,  0.1311, -0.0153,  0.1272, -0.1310,\n             0.1201, -0.0820,  0.0540,  0.0729, -0.0259, -0.0327],\n           [ 0.0620,  0.0305,  0.2059, -0.1042, -0.0801,  0.0403,  0.1350,\n            -0.1182, -0.1258, -0.0172,  0.0544, -0.0248,  0.1389, -0.0579,\n             0.1775, -0.0878, -0.0492,  0.0966, -0.0006, -0.0638],\n           [ 0.0556,  0.0323,  0.0837, -0.1329, -0.0739, -0.1043,  0.1628,\n            -0.0918, -0.1146, -0.0352,  0.1433, -0.0434,  0.1074, -0.1473,\n             0.1616, -0.1043,  0.0296, -0.0059, -0.0075, -0.0476]]],\n         grad_fn=<StackBackward>),\n  tensor([[[ 0.3485,  0.1324,  0.3720,  0.1400,  0.1260,  0.0992,  0.0639,\n            -0.3489, -0.4181, -0.1400, -0.2019, -0.2404, -0.2314, -0.0725,\n             0.2828,  0.4126, -0.2318,  0.1892, -0.1563,  0.0411],\n           [ 0.0551, -0.3473,  0.4002,  0.2760,  0.0231,  0.5432, -0.0946,\n            -0.0927,  0.0118,  0.0821, -0.3477,  0.2719, -0.4443, -0.2095,\n             0.2089,  0.0505, -0.3040,  0.0278, -0.1981,  0.1054],\n           [ 0.5039,  0.1157,  0.4123, -0.0480, -0.3321,  0.1314,  0.2115,\n             0.0428, -0.4316,  0.0629,  0.1119, -0.1781, -0.2011, -0.0457,\n             0.3124,  0.3689,  0.1203,  0.3621, -0.2793, -0.1010]],\n  \n          [[ 0.0592, -0.0102,  0.3246, -0.1957, -0.1222, -0.1503,  0.1168,\n            -0.1024, -0.2070,  0.1463,  0.2470, -0.0344,  0.2731, -0.2856,\n             0.2580, -0.2003,  0.1010,  0.1482, -0.0468, -0.0563],\n           [ 0.1311,  0.0555,  0.3790, -0.2026, -0.1994,  0.0983,  0.2444,\n            -0.2427, -0.2709, -0.0357,  0.0969, -0.0569,  0.2756, -0.1147,\n             0.3939, -0.1894, -0.0953,  0.2022, -0.0011, -0.1203],\n           [ 0.1181,  0.0621,  0.1498, -0.2702, -0.1849, -0.2313,  0.2830,\n            -0.1951, -0.2419, -0.0743,  0.2592, -0.0968,  0.2385, -0.3281,\n             0.3349, -0.2486,  0.0559, -0.0113, -0.0141, -0.0873]]],\n         grad_fn=<StackBackward>)))"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"wahrq13p5eihzj7g5los78"},"outputs":[],"execution_count":null}]}